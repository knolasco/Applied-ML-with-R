---
title: "Clustering with Hierarchical & DBSCAN methods"
author: "Kevin Nolasco"
date: "`r Sys.Date()`"
output: pdf_document
---

# Data

For this project, we will create an artificial dataset with 5 features generated randomly from normal, uniform, binomial, poisson, and exponential distributions. We will use PCA to project the data into two principal components and apply Hierarchical and DBSCAN clustering to the projected data.

```{r}
set.seed(1995)
n_observations <- 500
normal <- rnorm(n_observations, mean = 5, sd = 10)
poisson <- rpois(n_observations, 2)
binomial <- rbinom(n_observations, n_observations, 0.85)
uniform <- runif(n_observations, min = -20, max = 2)
exponential <- rexp(n_observations)

data <- data.frame(Normal = normal,
                   Poisson = poisson,
                   Binomial = binomial,
                   Uniform = uniform,
                   Exponential = exponential)
print(head(data))
```

```{r}
summary(data)
```

Now that we have created our dataset, let's visualize the distributions using historgrams followed by a scatter matrix.

## Histograms

```{r}
# histograms
par(mfrow = c(2, 3))
hist(data$Normal)
hist(data$Poisson)
hist(data$Uniform)
hist(data$Exponential)
hist(data$Binomial)
```

## Scatter Matrix

```{r}
pairs(data)
```

We can see from above that there is no clear relationship between any two variables that is easily discernible. Let's use PCA to see how well this data can be projected onto two dimensions.

## PCA

Let's use PCA to reduce the dimensionality of this dataset. The goal is to reduce it to 2 dimensions, but we will use the cummulative variance to decide how many principal components will be enough.

```{r}
model.pca_not_scaled <- prcomp(data)
model.pca_scaled <- prcomp(data, scale = TRUE)

print("Results for PCA not scaling the data")
print(summary(model.pca_not_scaled))

print("Results for PCA scaled")
print(summary(model.pca_scaled))
```


We can see that ~98% of the variance can be explained using the first three principal components for the PCA not scaled. Let's visualize the principal components.

```{r, message = FALSE, results = 'hide'}
library(devtools)
library(ggbiplot)

ggbiplot(model.pca_not_scaled)
```
It's a little difficult to see the components here, but we will take the first three as planned.

```{r}
data_tr <- model.pca_not_scaled$x[, c('PC1','PC2','PC3')]

# print first 5 rows for PCA dataset and original dataset
print(head(data_tr))
print(head(data))

```

## DBSCAN

Now that we have our transformed dataset we will use DBSCAN to classify the observations. Then we will plot it using the first 2 PCA's and see how the observations were grouped.

```{r}
library(fpc)
set.seed(1995)
db <- fpc::dbscan(data_tr, eps = 0.15, MinPts = 5)

plot(db, data_tr, main = "DBSCAN", frame = FALSE)
```

Above we can see the results of DBSCAN. We can see that the algorithm did not find any groups that it can call clusters. We will iteratively change the size of epsilon for the algorithm and see the results.

```{r}
library(glue)
set.seed(1995)
par(mfrow = c(2, 3))
for (eps_size in 1:6){
  db <- fpc::dbscan(data_tr, eps = eps_size, MinPts = 5)

  plot(db, data_tr, main = glue("DBSCAN using EPS = {eps_size}"), frame = FALSE)
}
```

We can see that the algorithm was able to find clusters for epsilon >= 2. Let's keep epsilon = 3 and iterate through the MinPts parameter.

```{r}
set.seed(1995)
par(mfrow = c(2, 3))
for (n_points in 1:6){
  db <- fpc::dbscan(data_tr, eps = 3, MinPts = n_points)

  plot(db, data_tr, main = glue("DBSCAN using MinPts = {n_points}"), frame = FALSE)
}
```

Let's choose MinPts = 3 and plot it in it's own.

```{r}
set.seed(1995)
db <- fpc::dbscan(data_tr, eps = 3, MinPts = 3)

plot(db, data_tr, main = "DBSCAN", frame = FALSE)
```

We can see that the alorithm was able to cluster this data into a few clusters. We can also see that the values that are far from the center were not clustered because they were not within epsilon of other points. 

## 
